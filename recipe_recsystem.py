# -*- coding: utf-8 -*-
"""recipe_recsystem.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10GDx4tNYXhnURpvi1qX8U1a-sP6MqTxN
"""

import numpy as np
import pandas as pd
import re
import nltk
import gensim.downloader as api
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.neighbors import NearestNeighbors
from scipy.stats import hmean
import json
#

nltk.download('stopwords')
nltk.download('wordnet')

glove_model = api.load("glove-wiki-gigaword-50")

recipes = pd.read_csv("recipes.csv")

recipes = recipes.sample(50000, random_state=42)  # Set random_state for reproducibility


word_vector_cache = {}

def get_vector(word):
    if word in word_vector_cache:
        return word_vector_cache[word]
    elif word in glove_model.key_to_index:
        word_vector_cache[word] = glove_model[word]
        return word_vector_cache[word]
    else:
        return np.zeros(50)

def get_recipe_vector(text):
    words = preprocess_text(text).split()
    vectors = [get_vector(word) for word in words if word in glove_model.key_to_index]
    return np.mean(vectors, axis=0) if vectors else np.zeros(50)

def preprocess_text(text):
    if pd.isna(text):
        return ''

    stop_words = set(stopwords.words('english'))
    lemmatizer = WordNetLemmatizer()

    text = str(text).lower()
    text = re.sub(r'[^\w\s]', '', text)

    words = text.split()

    processed_words = [
        lemmatizer.lemmatize(word)
        for word in words
        if word not in stop_words
    ]

    return ' '.join(processed_words)

def combine_features(row):
    features = []
    feature_columns = [
            'description',
            'ingredients_raw_str',
            'tags',
            'search_terms'
        ]
    for col in feature_columns:
            if col in row and not pd.isna(row[col]):
                features.append(str(row[col]))

    return ' '.join(features)

def prepare_recipe_data(recipes):
    recipes["combined_text"] = recipes.apply(combine_features, axis=1)
    recipes["processed_text"] = recipes["combined_text"].apply(preprocess_text)
    recipes["recipe_vector"] = recipes["combined_text"].apply(get_recipe_vector)
    return recipes

def create_tfidf_matrix(recipes):
    vectorizer = TfidfVectorizer()
    tfidf_matrix = vectorizer.fit_transform(recipes['processed_text'])
    return vectorizer, tfidf_matrix

recipes = prepare_recipe_data(recipes)
vectorizer, tfidf_matrix = create_tfidf_matrix(recipes)

recipe_vectors = np.vstack(recipes["recipe_vector"].values)
knn = NearestNeighbors(n_neighbors=5, metric='cosine')
knn.fit(recipe_vectors)

def recommend_knn(query, top_k=5):
    query_vector_glove = get_recipe_vector(query).reshape(1, -1)
    distances, indices = knn.kneighbors(query_vector_glove, n_neighbors=top_k)
    knn_similarities = 1 - distances[0]  # Convert distance to similarity

    recommendations = recipes.iloc[indices[0]][["name", "description"]]
    recommendations["similarity"] = knn_similarities
    return recommendations.to_dict(orient="records")

